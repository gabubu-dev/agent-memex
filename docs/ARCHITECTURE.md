# Memex Architecture

## Overview

Memex implements a **three-tier memory architecture** inspired by how human memory works:

1. **Knowledge Graph (Semantic Memory)** - Long-term facts about entities
2. **Daily Notes (Episodic Memory)** - Chronological event logs
3. **Tacit Knowledge (Procedural Memory)** - Operating principles and patterns

This document explains the design philosophy, implementation details, and usage patterns.

## Design Philosophy

### File-Based, Not Database

**Why files?**
- **Transparency:** Human-readable, grep-able, diff-able
- **Portability:** Works across platforms without dependencies
- **Simplicity:** No schema migrations, no database setup
- **Backup-friendly:** Git-compatible, syncable, repairable
- **Debuggable:** See exactly what the agent knows

**Trade-offs:**
- Slower than databases for massive datasets (but memory is small!)
- Manual indexing required (but automated via tools)
- Concurrent writes need care (but agents are single-threaded)

### Progressive Disclosure

**Problem:** Loading all search results wastes tokens.

**Solution:** Three-level retrieval:

```
Level 1: INDEX      (~100 tokens/result)  - Filter down to candidates
          â†“
Level 2: TIMELINE   (~300 tokens/result)  - Add chronological context
          â†“
Level 3: FULL       (~1000 tokens/result) - Load complete details
```

**Token savings:** 77% compared to loading everything at once.

**Example workflow:**
```bash
# Step 1: Get IDs (cheap)
python tools/memory-search-simple.py "Memex" --format index --limit 20
# â†’ 20 previews = ~2,000 tokens

# Step 2: Add timeline (medium)
python tools/memory-timeline.py --query "Memex" --before 48 --after 24
# â†’ Context = ~5,000 tokens

# Step 3: Load full details for 3 relevant items (expensive)
python tools/memory-search-simple.py --ids abc123,def456,ghi789
# â†’ 3 full entries = ~3,000 tokens

# Total: ~10,000 tokens vs. ~40,000 without progressive disclosure
```

## Three-Tier Architecture

### Layer 1: Knowledge Graph

**Purpose:** Store durable facts about entities that persist across sessions.

**Structure:**
```
~/life/areas/
â”œâ”€â”€ people/
â”‚   â”œâ”€â”€ alice/
â”‚   â”‚   â”œâ”€â”€ items.json       # Atomic facts (append-only)
â”‚   â”‚   â””â”€â”€ summary.md       # Current snapshot (regenerated)
â”‚   â””â”€â”€ bob/
â”‚       â”œâ”€â”€ items.json
â”‚       â””â”€â”€ summary.md
â”œâ”€â”€ companies/
â”‚   â””â”€â”€ anthropic/
â”‚       â”œâ”€â”€ items.json
â”‚       â””â”€â”€ summary.md
â”œâ”€â”€ projects/
â”‚   â””â”€â”€ memex/
â”‚       â”œâ”€â”€ items.json
â”‚       â””â”€â”€ summary.md
â”œâ”€â”€ skills/
â”‚   â””â”€â”€ python/
â”‚       â”œâ”€â”€ items.json
â”‚       â””â”€â”€ summary.md
â””â”€â”€ workflows/
    â””â”€â”€ deployment/
        â”œâ”€â”€ items.json
        â””â”€â”€ summary.md
```

**items.json format:**
```json
[
  {
    "id": "alice-001",
    "fact": "Alice is a senior engineer at Anthropic",
    "timestamp": "2026-01-30T10:30:00",
    "category": "employment",
    "source": "conversation",
    "supersedes": null
  },
  {
    "id": "alice-002",
    "fact": "Alice prefers async communication over meetings",
    "timestamp": "2026-01-30T14:20:00",
    "category": "preferences",
    "source": "observation",
    "supersedes": null
  }
]
```

**Key principles:**
- **Atomic facts:** One fact per item
- **Never delete:** Mark as superseded instead
- **Timestamped:** When was this learned?
- **Categorized:** employment, preferences, skills, etc.
- **Sourced:** Where did this come from?

**summary.md format:**
```markdown
# Alice

**Last updated:** 2026-01-30

## Overview
Alice is a senior engineer at Anthropic working on AI safety.

## Key Facts
- Employment: Senior Engineer at Anthropic (since 2024)
- Location: San Francisco
- Expertise: Python, AI safety, distributed systems

## Preferences
- Communication: Prefers async (Slack/email) over meetings
- Work style: Deep focus blocks, minimal interruptions
- Collaboration: Open to pairing on complex problems

## Current Projects
- Leading Claude memory system redesign
- Contributing to model evaluation framework
```

**Update frequency:** Weekly or when major changes occur

### Layer 2: Daily Notes

**Purpose:** Chronological event log of what happened each day.

**Structure:**
```
memory/
â”œâ”€â”€ 2026-01-30.md
â”œâ”€â”€ 2026-01-29.md
â”œâ”€â”€ 2026-01-28.md
â””â”€â”€ ...
```

**Format:**
```markdown
# 2026-01-30

## Morning

### 09:00 - Reviewed Memex architecture
- Decided on three-tier approach
- Inspiration from claude-mem's progressive disclosure
- File-based instead of SQLite for transparency

### 10:30 - Meeting with Alice
- Discussed AI safety considerations
- She recommended focusing on fact superseding
- Action: Implement atomic fact updates

## Afternoon

### 14:00 - Built memory-search-simple.py
- TF-IDF based search using scikit-learn
- Lighter than neural embeddings
- Index rebuilds in <5s for 1000 entries

### 16:30 - Tested progressive disclosure
- Measured 77% token savings on sample queries
- Index â†’ Timeline â†’ Full workflow works well
- Need to document the pattern

## Evening

### 20:00 - Started repository setup
- Created gabubu-dev/agent-memex
- Writing comprehensive README
- Goal: Ship by tomorrow morning
```

**Key principles:**
- **Chronological:** Events in order
- **Timestamped:** When did it happen?
- **Structured:** Use headers for time blocks
- **Action items:** Mark with `Action:` prefix
- **Decisions:** Mark with `Decided:` prefix

### Layer 3: Tacit Knowledge

**Purpose:** Operating principles, preferences, lessons learned.

**Files:**
- `MEMORY.md` - Long-term distilled wisdom
- `AGENTS.md` - How the agent operates
- `TOOLS.md` - Environment-specific notes
- `HEARTBEAT.md` - Periodic tasks and checks

**MEMORY.md example:**
```markdown
# Long-Term Memory

## Lessons Learned

### On Software Design
- File-based systems beat databases for transparency
- Progressive disclosure saves tokens (77% measured savings)
- Start simple, add complexity only when needed

### On Collaboration
- Async communication reduces interruptions
- Document decisions, not just code
- Make mental models explicit

## Preferences

### Communication Style
- Direct and concise
- Code examples over long explanations
- Ask for clarification when uncertain

### Development Workflow
- Test-driven when behavior is clear
- Prototype-driven when exploring
- Document as you build
```

**AGENTS.md example:**
```markdown
# Agent Operating Manual

## Core Principles

1. **Read before you act** - Check memory/YYYY-MM-DD.md for context
2. **Write it down** - No "mental notes", everything goes in files
3. **Ask when uncertain** - Better than guessing wrong
4. **Progressive disclosure** - Start with index, drill down as needed

## Daily Routine

### On Startup
1. Read MEMORY.md for long-term context
2. Read memory/YYYY-MM-DD.md (today + yesterday)
3. Check for pending action items

### During Sessions
1. Log important events to daily notes
2. Extract facts to knowledge graph (weekly batch)
3. Update MEMORY.md with lessons learned

### Before Shutdown
1. Compress long sessions if needed
2. Update knowledge graph if major learnings
3. Commit and push changes
```

## Search Implementation

### Indexing

**TF-IDF Approach (Simple):**
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Build vocabulary
vectorizer = TfidfVectorizer(
    max_features=5000,
    stop_words='english',
    ngram_range=(1, 2)
)

# Index all memory chunks
documents = [chunk.content for chunk in all_chunks]
matrix = vectorizer.fit_transform(documents)

# Search
query_vec = vectorizer.transform([query])
similarities = cosine_similarity(query_vec, matrix).flatten()
top_indices = similarities.argsort()[::-1][:limit]
```

**Neural Approach (Advanced):**
```python
from sentence_transformers import SentenceTransformer
import chromadb

# Load model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create Chroma collection
client = chromadb.PersistentClient(path="./vector_store")
collection = client.create_collection("memories")

# Add documents
for chunk in all_chunks:
    embedding = model.encode(chunk.content)
    collection.add(
        embeddings=[embedding.tolist()],
        documents=[chunk.content],
        metadatas=[chunk.metadata],
        ids=[chunk.id]
    )

# Search
query_embedding = model.encode(query)
results = collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=limit
)
```

### Chunking Strategy

**Problem:** Memory files can be long. How to split them?

**Solution:** Semantic chunking by section:

```python
def split_by_sections(content: str) -> List[str]:
    """Split markdown by headers."""
    # Split on headers
    parts = re.split(r'\n(?=#{1,3}\s)', content)
    
    # Each section becomes a searchable chunk
    chunks = []
    for part in parts:
        if len(part.strip()) < 50:  # Skip tiny chunks
            continue
        chunks.append(part.strip())
    
    return chunks
```

**Why sections?**
- Preserves context (header + content)
- Right size for search results (~200-500 words)
- Natural semantic boundaries

### Metadata Extraction

**Each chunk includes:**
```python
{
    'id': 'abc123',              # Unique identifier
    'content': '...',            # Full text
    'source': '/path/to/file',   # Where it came from
    'layer': 'daily',            # Which tier?
    'timestamp': '2026-01-30',   # When created/modified
    'entity': 'alice',           # Related entity (if applicable)
    'category': 'meeting'        # Type of content
}
```

**Enables filtering:**
```bash
# Only daily notes
--layer daily

# Only knowledge graph
--layer knowledge_graph

# Only about Alice
--entity alice

# Only recent
--since 2026-01-28
```

## Timeline View

**Purpose:** Show chronological context around a memory.

**How it works:**
1. Find anchor point (query result, date, or ID)
2. Set time window (e.g., 24h before, 24h after)
3. Gather all events in window
4. Sort chronologically
5. Display with visual markers

**Example:**
```
======================================================================
ğŸ“…  TIMELINE: 2026-01-29 â†’ 2026-01-31
    Window: 24h before, 24h after
======================================================================

ğŸ¯  ANCHOR POINT:
    Started building Memex memory system...

ğŸ“Š  15 EVENTS:

  â–¸ 2026-01-29
  ğŸ“ Reviewed claude-mem architecture
  ğŸ§  Decided on file-based approach [decision]

  â–¸ 2026-01-30  â† ANCHOR
  ğŸ“ Started building Memex
  ğŸ“ Implemented TF-IDF search
  ğŸ§  Measured 77% token savings [insight]

  â–¸ 2026-01-31
  ğŸ“ Wrote comprehensive documentation
  ğŸ“ Prepared Moltbook announcement
```

**Use cases:**
- "What was I working on around then?"
- "What led to this decision?"
- "What happened after this event?"

## Session Compression

**Purpose:** Compress long conversations into searchable summaries.

**Approach:** Rule-based extraction (no API calls)

**What it extracts:**

1. **Summary** (one-line overview)
   - First heading + key sentences
   - Max 500 chars

2. **Key Points** (important takeaways)
   - Bullet points
   - Emphasized text (**bold**, __underline__)
   - First sentences of sections
   - Limit: 20 points

3. **Action Items** (TODOs)
   - Pattern matching: "TODO:", "Action:", "need to", etc.
   - Checkboxes: `[ ]` or `[x]`
   - Limit: 10 items

4. **Decisions** (what was decided)
   - Pattern matching: "decided", "will", "going to"
   - Explicit decision markers
   - Limit: 10 decisions

5. **Entities** (people, projects, tools)
   - Proper noun detection (capitalized)
   - Known tech terms (Claude, GPT, Python, etc.)
   - Limit: 15 entities

**Output format:**
```json
{
  "timestamp": "2026-01-30T20:00:00",
  "source": "memory/2026-01-30.md",
  "original_tokens": 15000,
  "summary": "Built Memex memory system with 3-tier architecture | Key: file-based transparency; progressive disclosure; 77% token savings",
  "key_points": [
    "Decided on three-tier architecture (facts/events/wisdom)",
    "File-based instead of SQLite for transparency",
    "Progressive disclosure saves 77% tokens",
    "TF-IDF search is lighter than neural embeddings"
  ],
  "action_items": [
    "Write comprehensive README",
    "Create examples directory",
    "Prepare Moltbook announcement"
  ],
  "decisions": [
    "Use TF-IDF for simple version, offer neural as advanced",
    "Store compressed summaries in memory/summaries/",
    "Make progressive disclosure the default workflow"
  ],
  "entities_mentioned": [
    "Memex", "Claude", "Anthropic", "Clawdbot", "Alice"
  ],
  "compression_ratio": 0.033
}
```

**When to compress:**
- Daily files > 1000 tokens
- After long conversations
- Weekly reviews
- Before archiving old sessions

## Performance Characteristics

### Indexing Speed
- **TF-IDF:** ~1s for 1000 entries, ~5s for 10,000
- **Neural:** ~10s for 1000 entries (first load), ~20s for 10,000
- **Incremental:** Only re-index changed files

### Search Latency
- **TF-IDF:** <100ms for typical queries
- **Neural:** ~5s first query (model load), <100ms subsequent
- **Timeline:** <50ms (filesystem scan)

### Storage
- **Index (TF-IDF):** ~5MB for 10,000 entries
- **Index (Neural):** ~100MB for 10,000 entries
- **Compressed summaries:** ~10KB each

### Memory Usage
- **TF-IDF:** ~50MB RAM during search
- **Neural:** ~500MB RAM (model in memory)
- **Web viewer:** ~20MB RAM

## Integration Patterns

### Clawdbot Integration

```python
# In AGENTS.md heartbeat section
if HEARTBEAT:
    # Check if index needs update
    if memory_files_modified_since_last_index():
        os.system("python tools/memory/memory-search-simple.py --index")
    
    # Auto-compress long sessions
    if daily_file_tokens() > 5000:
        os.system("python tools/memory/memory-compress.py --recent 1")
```

### Claude Code Integration

```typescript
// MCP tool definition
tools: [
  {
    name: "search_memory",
    description: "Search agent memory with progressive disclosure",
    parameters: {
      query: { type: "string" },
      format: { type: "string", enum: ["index", "full"] },
      layer: { type: "string", enum: ["daily", "tacit", "knowledge_graph"] }
    },
    handler: (params) => {
      return exec(`python tools/memory-search-simple.py "${params.query}" --format ${params.format} --layer ${params.layer}`)
    }
  }
]
```

### Custom Assistant Integration

Just run the Python tools via subprocess - they're platform-agnostic!

## Future Enhancements

### Planned
- [ ] Automatic fact extraction from daily notes (NLP)
- [ ] Memory decay (fade old memories from search)
- [ ] Cross-reference suggestions
- [ ] Query expansion using LLM

### Under Consideration
- [ ] Multi-agent memory sharing
- [ ] Conflict resolution for concurrent writes
- [ ] Memory consolidation (merge similar facts)
- [ ] Automatic summary regeneration

## References

- **Inspiration:** [claude-mem](https://github.com/anthropics/claude-mem)
- **Search model:** [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
- **Vector DB:** [ChromaDB](https://www.trychroma.com/)
- **Design philosophy:** Human memory models (semantic, episodic, procedural)

---

*Built with â¤ï¸ for transparent, portable, agent-friendly memory.*
